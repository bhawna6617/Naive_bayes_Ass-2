{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a6768a",
   "metadata": {},
   "source": [
    "# quest 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a9f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the probability that an employee is a smoker given that they use the health insurance plan, we can use Bayes' theorem. Let's denote the events as follows:\n",
    "\n",
    "# A: The employee uses the health insurance plan.\n",
    "# B: The employee is a smoker.\n",
    "# We are given:\n",
    "\n",
    "# P(A): Probability that an employee uses the health insurance plan = 0.70 (70%)\n",
    "\n",
    "# P(B∣A): Probability that an employee is a smoker given that they use the health insurance plan = 0.40 (40%)\n",
    "# We want to find \n",
    "\n",
    "# P(B∣A), the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "# We are given:\n",
    "\n",
    "\n",
    "# P(A): Probability that an employee uses the health insurance plan = 0.70 (70%)\n",
    "\n",
    "# P(B∣A): Probability that an employee is a smoker given that they use the health insurance plan = 0.40 (40%)\n",
    "# We want to find \n",
    "\n",
    "# P(B∣A), the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "# Bayes' theorem states:\n",
    "# P(B∣A)= \n",
    "# P(A)\n",
    "# P(A∣B)×P(B)\n",
    "\n",
    "# We have \n",
    "\n",
    "# P(A)=0.70 and \n",
    "\n",
    "# 0.40\n",
    "# P(B∣A)=0.40.\n",
    "\n",
    "# We know that:\n",
    "\n",
    "# P(B) is the overall probability that an employee is a smoker, regardless of whether they use the health insurance plan or not. We don't have this information directly from the given data.\n",
    "\n",
    "# P(A∣B) is the probability that an employee uses the health insurance plan given that they are a smoker. This is not provided in the given information.\n",
    "# Without the probability \n",
    "\n",
    "# P(B) or \n",
    "\n",
    "# P(A∣B), we can't directly calculate \n",
    "\n",
    "# P(B∣A) using Bayes' theorem with the given information. We would need additional data or assumptions to proceed with the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaaae5b",
   "metadata": {},
   "source": [
    "# quest 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba10677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes:\n",
    "# Feature Type: Bernoulli Naive Bayes is suitable for binary feature data, where features represent binary occurrences (e.g., presence or absence of a term in a document).\n",
    "# Probability Distribution: It assumes that features are generated from a Bernoulli distribution, where each feature is considered a binary random variable.\n",
    "# Application: Bernoulli Naive Bayes is commonly used in text classification tasks, such as sentiment analysis or spam detection, where features represent the presence or absence of words or terms in documents.\n",
    "# Multinomial Naive Bayes:\n",
    "# Feature Type: Multinomial Naive Bayes is designed for features that represent counts or frequencies, typically encountered in text data, where features are word counts or term frequencies.\n",
    "# Probability Distribution: It assumes that features are generated from a Multinomial distribution, where each feature represents the frequency of occurrence of a term.\n",
    "# Application: Multinomial Naive Bayes is widely used in text classification tasks, especially in document classification or topic modeling, where features are represented by word frequencies or TF-IDF (Term Frequency-Inverse Document Frequency) scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b0ad0",
   "metadata": {},
   "source": [
    "# quest 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2e4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes typically handles missing values by treating them as a specific category or by imputing them with a particular value before applying the classification algorithm. Here are two common approaches:\n",
    "\n",
    "# Treating Missing Values as a Separate Category:\n",
    "# One common approach is to treat missing values as a separate category during feature extraction. In the context of Bernoulli Naive Bayes, this means representing missing values as a binary indicator variable, indicating whether the feature is missing or not.\n",
    "# For example, if a feature represents the presence or absence of a term in a document, missing values can be represented as a separate category, indicating that the term is neither present nor absent in the document.\n",
    "# When computing probabilities during classification, the model considers the presence of missing values as one of the possible outcomes and updates the probabilities accordingly.\n",
    "# Imputation of Missing Values:\n",
    "# Another approach is to impute missing values with a specific value before applying the classification algorithm. This value can be determined based on domain knowledge or by using techniques such as mean imputation, mode imputation, or using a learned value from the training data.\n",
    "# For example, missing binary features might be imputed with the mode of the feature (the most common value) or with a value that represents uncertainty, such as -1.\n",
    "# After imputation, the dataset can be treated as complete, and standard Bernoulli Naive Bayes can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb2e906",
   "metadata": {},
   "source": [
    "# quest 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yes, Gaussian Naive Bayes can be used for multi-class classification. In Gaussian Naive Bayes, the features are assumed to follow a Gaussian (normal) distribution, and it's commonly used for continuous-valued features.\n",
    "\n",
    "# For multi-class classification, Gaussian Naive Bayes extends the basic binary classification approach to handle multiple classes. The classifier calculates the likelihood of each class given the observed feature values and selects the class with the highest likelihood as the predicted class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
